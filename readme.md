# 前沿
之前做了个博弈类的强化学习，奈何游戏选的不好，游戏机制的漏洞很快被网络捕获，导致没法深入研究，此次就做一个单独和环境交互的游戏，来进行强化学习的一些探索。

# 游戏环境
+ game:2048
+ step: ↑ ↓ ← → ，return s,r,done,s'
+ r:每回合得分，死掉扣1000

# 模型测试
## 随机版本
+ 随机进行1000次,得分最高2500左右，结局大致如下，还是比较菜的一个版本。
    2	 4	    16	    2

    32	 128	64	    8

    16	 2	    16	    2

    4	 8	    256	    8

## DQN2013版
Q1: 结果取向与走不动
A1:
+ 训练集剔除走不动的s-s'，这样容易让模型趋于走不动的地步。

Q1：对于初始学习能力较弱，4*4直接喂给dnn,且初始数据不足的情况下，没有能学习到尽可能去合并的概念。
A1：
+ 尝试增强特征提取，行列分开处理。
+ 尝试优化reward，尽可能保留更多的空格

# 总结
+ 对于2048来讲，是个模型已知的游戏，为何得出此结论？
    + P（s'|s,a） 是已知的一个随机事件，可模拟。
    + R（s'|s,a）是已知
    + 整个环境是可模拟的，那么也就可以通过值迭代和策略迭代的方法来完成，但是有16个位置，每个位置算10种可能，16^10的可能性也太多了，9^9也太多了，4^4还可以接受，那么我们
    还是尝试从2*2的方格开始解决这个问题。

# todo_list
2019.11.20 完成2*2的值迭代、策略迭代版本
todo : 完成MC/Q-LEARNING版本